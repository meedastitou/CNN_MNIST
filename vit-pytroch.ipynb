{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":242592,"sourceType":"datasetVersion","datasetId":102285}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/mohamedastitou/vit-pytroch?scriptVersionId=209794644\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"pip install idx2numpy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T18:57:03.248513Z","iopub.execute_input":"2024-11-26T18:57:03.249451Z","iopub.status.idle":"2024-11-26T18:57:11.220002Z","shell.execute_reply.started":"2024-11-26T18:57:03.249401Z","shell.execute_reply":"2024-11-26T18:57:11.218824Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: idx2numpy in /opt/conda/lib/python3.10/site-packages (1.2.3)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from idx2numpy) (1.26.4)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from idx2numpy) (1.16.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"import numpy as np\n\nfrom tqdm import tqdm, trange\nimport idx2numpy\nimport torch\nimport torch.nn as nn\nfrom torch.optim import Adam\nfrom torch.nn import CrossEntropyLoss\nfrom torch.utils.data import DataLoader, Subset, Dataset\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T18:59:07.976823Z","iopub.execute_input":"2024-11-26T18:59:07.977195Z","iopub.status.idle":"2024-11-26T18:59:07.981819Z","shell.execute_reply.started":"2024-11-26T18:59:07.977163Z","shell.execute_reply":"2024-11-26T18:59:07.98097Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"from torchvision.transforms import ToTensor\nfrom torchvision.datasets.mnist import MNIST","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T18:59:13.6405Z","iopub.execute_input":"2024-11-26T18:59:13.641451Z","iopub.status.idle":"2024-11-26T18:59:13.644998Z","shell.execute_reply.started":"2024-11-26T18:59:13.641413Z","shell.execute_reply":"2024-11-26T18:59:13.644166Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"# Loading data\n\n# transform = ToTensor()\n\n# train_set = MNIST(root='/kaggle/input/', train=True, download=True, transform=transform)\n# test_set = MNIST(root='/kaggle/input/', train=False, download=True, transform=transform)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T18:59:14.123419Z","iopub.execute_input":"2024-11-26T18:59:14.123696Z","iopub.status.idle":"2024-11-26T18:59:14.127523Z","shell.execute_reply.started":"2024-11-26T18:59:14.123671Z","shell.execute_reply":"2024-11-26T18:59:14.126622Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"#train_subset = Subset(train_set, indices=list(range(500)))\n#test_subset = Subset(test_set, indices=list(range(100)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T18:59:14.726741Z","iopub.execute_input":"2024-11-26T18:59:14.727055Z","iopub.status.idle":"2024-11-26T18:59:14.730416Z","shell.execute_reply.started":"2024-11-26T18:59:14.727026Z","shell.execute_reply":"2024-11-26T18:59:14.729575Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"# Charger les donn√©es IDX\ntrain_images_path = \"/kaggle/input/mnist-dataset/train-images.idx3-ubyte\"\ntrain_labels_path = \"/kaggle/input/mnist-dataset/train-labels.idx1-ubyte\"\ntest_images_path = \"/kaggle/input/mnist-dataset/t10k-images.idx3-ubyte\"\ntest_labels_path = \"/kaggle/input/mnist-dataset/t10k-labels.idx1-ubyte\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T18:59:15.198603Z","iopub.execute_input":"2024-11-26T18:59:15.199021Z","iopub.status.idle":"2024-11-26T18:59:15.203722Z","shell.execute_reply.started":"2024-11-26T18:59:15.198989Z","shell.execute_reply":"2024-11-26T18:59:15.202796Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"train_images = idx2numpy.convert_from_file(train_images_path)\ntrain_labels = idx2numpy.convert_from_file(train_labels_path)\ntest_images = idx2numpy.convert_from_file(test_images_path)\ntest_labels = idx2numpy.convert_from_file(test_labels_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T18:59:15.798016Z","iopub.execute_input":"2024-11-26T18:59:15.798679Z","iopub.status.idle":"2024-11-26T18:59:15.848032Z","shell.execute_reply.started":"2024-11-26T18:59:15.798645Z","shell.execute_reply":"2024-11-26T18:59:15.84711Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"print(train_images.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T18:59:16.208868Z","iopub.execute_input":"2024-11-26T18:59:16.209693Z","iopub.status.idle":"2024-11-26T18:59:16.214007Z","shell.execute_reply.started":"2024-11-26T18:59:16.20966Z","shell.execute_reply":"2024-11-26T18:59:16.213108Z"}},"outputs":[{"name":"stdout","text":"(60000, 28, 28)\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"#custionm mnist dataset to vgg\nclass mnist_dataset(Dataset):\n    def __init__(self, images, labels, transform=None):\n        self.images = images\n        self.labels = labels\n        self.transform = transform\n\n\n    def __len__(self):\n        return len(self.images)\n\n   \n    def __getitem__(self, idx):\n        image = self.images[idx]\n        label = self.labels[idx]\n        if self.transform:\n            image = self.transform(image)\n        return image, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T18:59:16.548882Z","iopub.execute_input":"2024-11-26T18:59:16.549174Z","iopub.status.idle":"2024-11-26T18:59:16.554467Z","shell.execute_reply.started":"2024-11-26T18:59:16.549148Z","shell.execute_reply":"2024-11-26T18:59:16.553565Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"train_dataset = mnist_dataset(train_images, train_labels, transform=ToTensor())\ntest_dataset = mnist_dataset(test_images, test_labels, transform=ToTensor())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T18:59:16.925539Z","iopub.execute_input":"2024-11-26T18:59:16.925813Z","iopub.status.idle":"2024-11-26T18:59:16.929937Z","shell.execute_reply.started":"2024-11-26T18:59:16.925786Z","shell.execute_reply":"2024-11-26T18:59:16.928949Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, shuffle=True, batch_size=128)\ntest_loader = DataLoader(test_dataset, shuffle=False, batch_size=128)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T18:59:17.339159Z","iopub.execute_input":"2024-11-26T18:59:17.339657Z","iopub.status.idle":"2024-11-26T18:59:17.349275Z","shell.execute_reply.started":"2024-11-26T18:59:17.339629Z","shell.execute_reply":"2024-11-26T18:59:17.348423Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T18:59:17.92788Z","iopub.execute_input":"2024-11-26T18:59:17.928201Z","iopub.status.idle":"2024-11-26T18:59:17.93273Z","shell.execute_reply.started":"2024-11-26T18:59:17.928175Z","shell.execute_reply":"2024-11-26T18:59:17.931942Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"def patchify(images, n_patches):\n    n, c, h, w = images.shape\n\n    assert h == w, \"Patchify method is implemented for square images only\"\n\n    patches = torch.zeros(n, n_patches ** 2, h * w * c // n_patches ** 2)\n    patch_size = h // n_patches\n\n    for idx, image in enumerate(images):\n        for i in range(n_patches):\n            for j in range(n_patches):\n                patch = image[:, i * patch_size: (i + 1) * patch_size, j * patch_size: (j + 1) * patch_size]\n                patches[idx, i * n_patches + j] = patch.flatten()\n    return patches","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T18:59:18.77688Z","iopub.execute_input":"2024-11-26T18:59:18.77753Z","iopub.status.idle":"2024-11-26T18:59:18.783005Z","shell.execute_reply.started":"2024-11-26T18:59:18.777499Z","shell.execute_reply":"2024-11-26T18:59:18.782156Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"def get_positional_embeddings(sequence_length, d):\n    print(\"h\")\n\n    result = torch.ones(sequence_length, d)\n    for i in range(sequence_length):\n        for j in range(d):\n            result[i][j] = np.sin(i / (10000 ** (j / d))) if j % 2 == 0 else np.cos(i / (10000 ** ((j - 1) / d)))\n    return result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T18:59:19.236392Z","iopub.execute_input":"2024-11-26T18:59:19.237186Z","iopub.status.idle":"2024-11-26T18:59:19.241568Z","shell.execute_reply.started":"2024-11-26T18:59:19.237153Z","shell.execute_reply":"2024-11-26T18:59:19.240703Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"class MyViT(nn.Module):\n  def __init__(self, chw, n_patches=7, n_blocks=2, hidden_d=8, n_heads=2, out_d=10):\n    # Super constructor\n    super(MyViT, self).__init__()\n    \n    # Attributes\n    self.chw = chw # ( C , H , W )\n    self.n_patches = n_patches\n    self.n_blocks = n_blocks\n    self.n_heads = n_heads\n    self.hidden_d = hidden_d\n        \n    assert chw[1] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"\n    assert chw[2] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"\n    self.patch_size = (chw[1] / n_patches, chw[2] / n_patches)\n\n    # 1) Linear mapper\n    self.input_d = int(chw[0] * self.patch_size[0] * self.patch_size[1])\n    self.linear_mapper = nn.Linear(self.input_d, self.hidden_d)\n\n    # 2) Learnable classifiation token\n    self.class_token = nn.Parameter(torch.rand(1, self.hidden_d))\n      \n    #print(self.class_token)\n    # 3) Positional embedding\n    # self.pos_embed = nn.Parameter(torch.tensor(get_positional_embeddings(self.n_patches ** 2 + 1, self.hidden_d)))\n    # self.pos_embed.requires_grad = False\n    self.register_buffer('positional_embeddings', get_positional_embeddings(n_patches ** 2 + 1, hidden_d), persistent=False)\n\n    # 4) Transformer encoder blocks\n    self.blocks = nn.ModuleList([MyViTBlock(hidden_d, n_heads) for _ in range(n_blocks)])\n\n    # 5) Classification MLPk\n    self.mlp = nn.Sequential(\n        nn.Linear(self.hidden_d, 128),\n        nn.ReLU(),\n        nn.Linear(128, 64),\n        nn.ReLU(),\n        nn.Linear(64, 32),\n        nn.ReLU(),\n        nn.Linear(32, out_d),\n        nn.Softmax(dim=-1)\n    )\n\n  def forward(self, images):\n    n, c, h, w = images.shape\n      \n    patches = patchify(images, self.n_patches).to(self.positional_embeddings.device)\n      \n    tokens = self.linear_mapper(patches)\n\n      \n    # Adding classification token to the tokens\n    tokens = torch.cat((self.class_token.expand(n, 1, -1), tokens), dim=1)\n\n    # tokens = torch.stack([torch.vstack((self.class_token, tokens[i])) for i in range(len(tokens))])\n    #print(tokens[0])\n    # Adding positional embedding\n    # pos_embed = self.pos_embed.repeat(2, 1, 1)\n    #print(pos_embed)\n    # out = tokens + pos_embed\n    out = tokens + self.positional_embeddings.repeat(n, 1, 1)\n    for block in self.blocks:\n            out = block(out)\n    # Getting the classification token only\n    out = out[:, 0]\n    return self.mlp(out) # Map to output dimension, output category distribution\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T18:59:19.740079Z","iopub.execute_input":"2024-11-26T18:59:19.740356Z","iopub.status.idle":"2024-11-26T18:59:19.749809Z","shell.execute_reply.started":"2024-11-26T18:59:19.740329Z","shell.execute_reply":"2024-11-26T18:59:19.748841Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"class MyMSA(nn.Module):\n    def __init__(self, d, n_heads=2):\n        super(MyMSA, self).__init__()\n        self.d = d\n        self.n_heads = n_heads\n\n        assert d % n_heads == 0, f\"Can't divide dimension {d} into {n_heads} heads\"\n        d_head = int(d / n_heads)\n        \n        print(\"d_head\", d_head)\n\n        self.q_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n        self.k_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n        self.v_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n        self.d_head = d_head\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, sequences):\n        # Sequences has shape (N, seq_length, token_dim)\n        # We go into shape    (N, seq_length, n_heads, token_dim / n_heads)\n        # And come back to    (N, seq_length, item_dim)  (through concatenation)\n        # print(\"sequences\" , sequences)\n        result = []\n        for sequence in sequences:\n            # print(\"sequence\", sequence)\n            seq_result = []\n            for head in range(self.n_heads):\n                q_mapping = self.q_mappings[head]\n                k_mapping = self.k_mappings[head]\n                v_mapping = self.v_mappings[head]\n\n                seq = sequence[:, head * self.d_head: (head + 1) * self.d_head]\n                # print(\"seq\", seq)\n                q, k, v = q_mapping(seq), k_mapping(seq), v_mapping(seq)\n                # weights = list(q_mapping.parameters())[0]\n\n                # print(weights.shape)  # Output: torch.Size([5, 10])\n                # print(weights)  # Output: tensor containing the weight values\n                # print(q)\n                # print(k)\n                # print(k.T)\n                attention = self.softmax(q @ k.T / (self.d_head ** 0.5))\n                seq_result.append(attention @ v)\n                # print(\"seq_result\", seq_result)\n            result.append(torch.hstack(seq_result))\n            # print(\"resutl\", result)\n        return torch.cat([torch.unsqueeze(r, dim=0) for r in result])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T18:59:20.181779Z","iopub.execute_input":"2024-11-26T18:59:20.182068Z","iopub.status.idle":"2024-11-26T18:59:20.190386Z","shell.execute_reply.started":"2024-11-26T18:59:20.182041Z","shell.execute_reply":"2024-11-26T18:59:20.189546Z"}},"outputs":[],"execution_count":53},{"cell_type":"code","source":"class MyViTBlock(nn.Module):\n    def __init__(self, hidden_d, n_heads, mlp_ratio=4):\n        super(MyViTBlock, self).__init__()\n        self.hidden_d = hidden_d\n        self.n_heads = n_heads\n\n        self.norm1 = nn.LayerNorm(hidden_d)\n        self.mhsa = MyMSA(hidden_d, n_heads)\n        self.norm2 = nn.LayerNorm(hidden_d)\n        self.mlp = nn.Sequential(\n            nn.Linear(hidden_d, mlp_ratio * hidden_d),\n            nn.GELU(),\n            nn.Linear(mlp_ratio * hidden_d, hidden_d)\n        )\n\n    def forward(self, x):\n        out = x + self.mhsa(self.norm1(x))\n        out = out + self.mlp(self.norm2(out))\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T18:59:20.7554Z","iopub.execute_input":"2024-11-26T18:59:20.755679Z","iopub.status.idle":"2024-11-26T18:59:20.761428Z","shell.execute_reply.started":"2024-11-26T18:59:20.755654Z","shell.execute_reply":"2024-11-26T18:59:20.760629Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"# if __name__ == '__main__':\n#     model = MyViTBlock(hidden_d=4, n_heads=2)\n\n#     x = torch.randn(3, 4, 4)  # Dummy sequences\n#     # print(x)\n#     print(model(x).shape)      ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T18:59:21.470328Z","iopub.execute_input":"2024-11-26T18:59:21.470595Z","iopub.status.idle":"2024-11-26T18:59:21.474465Z","shell.execute_reply.started":"2024-11-26T18:59:21.470571Z","shell.execute_reply":"2024-11-26T18:59:21.473603Z"}},"outputs":[],"execution_count":55},{"cell_type":"code","source":"model = MyViT((1, 28, 28), n_patches=7, n_blocks=2, hidden_d=16, n_heads=2, out_d=10).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T18:59:22.378498Z","iopub.execute_input":"2024-11-26T18:59:22.378816Z","iopub.status.idle":"2024-11-26T18:59:22.397705Z","shell.execute_reply.started":"2024-11-26T18:59:22.378787Z","shell.execute_reply":"2024-11-26T18:59:22.396973Z"}},"outputs":[{"name":"stdout","text":"h\nd_head 8\nd_head 8\n","output_type":"stream"}],"execution_count":56},{"cell_type":"code","source":"N_EPOCHS = 30\nLR = 0.01\n\n# Training loop\noptimizer = Adam(model.parameters(), lr=LR)\ncriterion = CrossEntropyLoss()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T18:59:23.464417Z","iopub.execute_input":"2024-11-26T18:59:23.46475Z","iopub.status.idle":"2024-11-26T18:59:23.470346Z","shell.execute_reply.started":"2024-11-26T18:59:23.464721Z","shell.execute_reply":"2024-11-26T18:59:23.469246Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"for epoch in range(N_EPOCHS):\n        model.train()\n        train_loss = 0.0\n        for batch in tqdm(train_loader, desc=f\" the {epoch+1}/{N_EPOCHS} training\"):\n            x, y = batch\n            x, y = x.to(device), y.to(device)\n            predit = model(x)\n            loss = criterion(predit, y)\n\n            train_loss += loss.detach().cpu().item() \n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        print(f\"Epoch {epoch + 1}/{N_EPOCHS} loss: {train_loss/len(train_loader):.6f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T19:01:48.851961Z","iopub.execute_input":"2024-11-26T19:01:48.852744Z"}},"outputs":[{"name":"stderr","text":" the 1/30 training:   0%|          | 0/469 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:154: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /usr/local/src/pytorch/torch/csrc/utils/tensor_numpy.cpp:206.)\n  img = torch.from_numpy(pic.transpose((2, 0, 1))).contiguous()\n the 1/30 training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 469/469 [03:12<00:00,  2.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/30 loss: 2.301727\n","output_type":"stream"},{"name":"stderr","text":" the 2/30 training:  18%|‚ñà‚ñä        | 83/469 [00:34<02:44,  2.35it/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}