{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":242592,"sourceType":"datasetVersion","datasetId":102285}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/mohamedastitou/vit-pytroch?scriptVersionId=209794644\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"pip install idx2numpy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T18:40:55.226991Z","iopub.execute_input":"2024-11-26T18:40:55.227434Z","iopub.status.idle":"2024-11-26T18:41:05.904397Z","shell.execute_reply.started":"2024-11-26T18:40:55.227387Z","shell.execute_reply":"2024-11-26T18:41:05.903341Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\nfrom tqdm import tqdm, trange\nimport idx2numpy\nimport torch\nimport torch.nn as nn\nfrom torch.optim import Adam\nfrom torch.nn import CrossEntropyLoss\nfrom torch.utils.data import DataLoader, Subset, Dataset\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T18:41:05.906061Z","iopub.execute_input":"2024-11-26T18:41:05.906312Z","iopub.status.idle":"2024-11-26T18:41:08.407401Z","shell.execute_reply.started":"2024-11-26T18:41:05.906288Z","shell.execute_reply":"2024-11-26T18:41:08.406709Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchvision.transforms import ToTensor\nfrom torchvision.datasets.mnist import MNIST","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T18:41:08.408318Z","iopub.execute_input":"2024-11-26T18:41:08.408631Z","iopub.status.idle":"2024-11-26T18:41:09.399495Z","shell.execute_reply.started":"2024-11-26T18:41:08.408607Z","shell.execute_reply":"2024-11-26T18:41:09.398748Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Loading data\n\n# transform = ToTensor()\n\n# train_set = MNIST(root='/kaggle/input/', train=True, download=True, transform=transform)\n# test_set = MNIST(root='/kaggle/input/', train=False, download=True, transform=transform)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T18:43:08.473657Z","iopub.execute_input":"2024-11-26T18:43:08.474104Z","iopub.status.idle":"2024-11-26T18:43:08.47778Z","shell.execute_reply.started":"2024-11-26T18:43:08.474071Z","shell.execute_reply":"2024-11-26T18:43:08.476915Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#train_subset = Subset(train_set, indices=list(range(500)))\n#test_subset = Subset(test_set, indices=list(range(100)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T18:43:08.971433Z","iopub.execute_input":"2024-11-26T18:43:08.971689Z","iopub.status.idle":"2024-11-26T18:43:08.975358Z","shell.execute_reply.started":"2024-11-26T18:43:08.971665Z","shell.execute_reply":"2024-11-26T18:43:08.97431Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Charger les donn√©es IDX\ntrain_images_path = \"/kaggle/input/mnist-dataset/train-images.idx3-ubyte\"\ntrain_labels_path = \"/kaggle/input/mnist-dataset/train-labels.idx1-ubyte\"\ntest_images_path = \"/kaggle/input/mnist-dataset/t10k-images.idx3-ubyte\"\ntest_labels_path = \"/kaggle/input/mnist-dataset/t10k-labels.idx1-ubyte\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T18:43:09.635885Z","iopub.execute_input":"2024-11-26T18:43:09.636556Z","iopub.status.idle":"2024-11-26T18:43:09.640409Z","shell.execute_reply.started":"2024-11-26T18:43:09.636525Z","shell.execute_reply":"2024-11-26T18:43:09.639486Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_images = idx2numpy.convert_from_file(train_images_path)\ntrain_labels = idx2numpy.convert_from_file(train_labels_path)\ntest_images = idx2numpy.convert_from_file(test_images_path)\ntest_labels = idx2numpy.convert_from_file(test_labels_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T18:43:10.68986Z","iopub.execute_input":"2024-11-26T18:43:10.690193Z","iopub.status.idle":"2024-11-26T18:43:10.738391Z","shell.execute_reply.started":"2024-11-26T18:43:10.690166Z","shell.execute_reply":"2024-11-26T18:43:10.737734Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(train_images.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T18:43:12.610806Z","iopub.execute_input":"2024-11-26T18:43:12.611165Z","iopub.status.idle":"2024-11-26T18:43:12.616366Z","shell.execute_reply.started":"2024-11-26T18:43:12.611135Z","shell.execute_reply":"2024-11-26T18:43:12.615338Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#custionm mnist dataset to vgg\nclass mnist_dataset(Dataset):\n    def __init__(self, images, labels, transform=None):\n        self.images = images\n        self.labels = labels\n        self.transform = transform\n\n\n    def __len__(self):\n        return len(self.images)\n\n   \n    def __getitem__(self, idx):\n        image = self.images[idx]\n        label = self.labels[idx]\n        if self.transform:\n            image = self.transform(image)\n        return image, label","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T18:43:14.419088Z","iopub.execute_input":"2024-11-26T18:43:14.41967Z","iopub.status.idle":"2024-11-26T18:43:14.424734Z","shell.execute_reply.started":"2024-11-26T18:43:14.419641Z","shell.execute_reply":"2024-11-26T18:43:14.423843Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = mnist_dataset(train_images, train_labels, transform=ToTensor())\ntest_dataset = mnist_dataset(test_images, test_labels, transform=ToTensor())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T18:43:17.614163Z","iopub.execute_input":"2024-11-26T18:43:17.614734Z","iopub.status.idle":"2024-11-26T18:43:17.623982Z","shell.execute_reply.started":"2024-11-26T18:43:17.614704Z","shell.execute_reply":"2024-11-26T18:43:17.623177Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, shuffle=True, batch_size=128)\ntest_loader = DataLoader(test_dataset, shuffle=False, batch_size=128)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def patchify(images, n_patches):\n    n, c, h, w = images.shape\n\n    assert h == w, \"Patchify method is implemented for square images only\"\n\n    patches = torch.zeros(n, n_patches ** 2, h * w * c // n_patches ** 2)\n    patch_size = h // n_patches\n\n    for idx, image in enumerate(images):\n        for i in range(n_patches):\n            for j in range(n_patches):\n                patch = image[:, i * patch_size: (i + 1) * patch_size, j * patch_size: (j + 1) * patch_size]\n                patches[idx, i * n_patches + j] = patch.flatten()\n    return patches","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_positional_embeddings(sequence_length, d):\n    print(\"h\")\n\n    result = torch.ones(sequence_length, d)\n    for i in range(sequence_length):\n        for j in range(d):\n            result[i][j] = np.sin(i / (10000 ** (j / d))) if j % 2 == 0 else np.cos(i / (10000 ** ((j - 1) / d)))\n    return result","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MyViT(nn.Module):\n  def __init__(self, chw, n_patches=7, n_blocks=2, hidden_d=8, n_heads=2, out_d=10):\n    # Super constructor\n    super(MyViT, self).__init__()\n    \n    # Attributes\n    self.chw = chw # ( C , H , W )\n    self.n_patches = n_patches\n    self.n_blocks = n_blocks\n    self.n_heads = n_heads\n    self.hidden_d = hidden_d\n        \n    assert chw[1] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"\n    assert chw[2] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"\n    self.patch_size = (chw[1] / n_patches, chw[2] / n_patches)\n\n    # 1) Linear mapper\n    self.input_d = int(chw[0] * self.patch_size[0] * self.patch_size[1])\n    self.linear_mapper = nn.Linear(self.input_d, self.hidden_d)\n\n    # 2) Learnable classifiation token\n    self.class_token = nn.Parameter(torch.rand(1, self.hidden_d))\n      \n    #print(self.class_token)\n    # 3) Positional embedding\n    # self.pos_embed = nn.Parameter(torch.tensor(get_positional_embeddings(self.n_patches ** 2 + 1, self.hidden_d)))\n    # self.pos_embed.requires_grad = False\n    self.register_buffer('positional_embeddings', get_positional_embeddings(n_patches ** 2 + 1, hidden_d), persistent=False)\n\n    # 4) Transformer encoder blocks\n    self.blocks = nn.ModuleList([MyViTBlock(hidden_d, n_heads) for _ in range(n_blocks)])\n\n    # 5) Classification MLPk\n    self.mlp = nn.Sequential(\n        nn.Linear(self.hidden_d, 128),\n        nn.ReLU(),\n        nn.Linear(128, 64),\n        nn.ReLU(),\n        nn.Linear(64, 32),\n        nn.ReLU(),\n        nn.Linear(32, out_d),\n        nn.Softmax(dim=-1)\n    )\n\n  def forward(self, images):\n    n, c, h, w = images.shape\n      \n    patches = patchify(images, self.n_patches).to(self.positional_embeddings.device)\n      \n    tokens = self.linear_mapper(patches)\n\n      \n    # Adding classification token to the tokens\n    tokens = torch.cat((self.class_token.expand(n, 1, -1), tokens), dim=1)\n\n    # tokens = torch.stack([torch.vstack((self.class_token, tokens[i])) for i in range(len(tokens))])\n    #print(tokens[0])\n    # Adding positional embedding\n    # pos_embed = self.pos_embed.repeat(2, 1, 1)\n    #print(pos_embed)\n    # out = tokens + pos_embed\n    out = tokens + self.positional_embeddings.repeat(n, 1, 1)\n    for block in self.blocks:\n            out = block(out)\n    # Getting the classification token only\n    out = out[:, 0]\n    return self.mlp(out) # Map to output dimension, output category distribution\n    ","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MyMSA(nn.Module):\n    def __init__(self, d, n_heads=2):\n        super(MyMSA, self).__init__()\n        self.d = d\n        self.n_heads = n_heads\n\n        assert d % n_heads == 0, f\"Can't divide dimension {d} into {n_heads} heads\"\n        d_head = int(d / n_heads)\n        \n        print(\"d_head\", d_head)\n\n        self.q_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n        self.k_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n        self.v_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n        self.d_head = d_head\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, sequences):\n        # Sequences has shape (N, seq_length, token_dim)\n        # We go into shape    (N, seq_length, n_heads, token_dim / n_heads)\n        # And come back to    (N, seq_length, item_dim)  (through concatenation)\n        # print(\"sequences\" , sequences)\n        result = []\n        for sequence in sequences:\n            # print(\"sequence\", sequence)\n            seq_result = []\n            for head in range(self.n_heads):\n                q_mapping = self.q_mappings[head]\n                k_mapping = self.k_mappings[head]\n                v_mapping = self.v_mappings[head]\n\n                seq = sequence[:, head * self.d_head: (head + 1) * self.d_head]\n                # print(\"seq\", seq)\n                q, k, v = q_mapping(seq), k_mapping(seq), v_mapping(seq)\n                # weights = list(q_mapping.parameters())[0]\n\n                # print(weights.shape)  # Output: torch.Size([5, 10])\n                # print(weights)  # Output: tensor containing the weight values\n                # print(q)\n                # print(k)\n                # print(k.T)\n                attention = self.softmax(q @ k.T / (self.d_head ** 0.5))\n                seq_result.append(attention @ v)\n                # print(\"seq_result\", seq_result)\n            result.append(torch.hstack(seq_result))\n            # print(\"resutl\", result)\n        return torch.cat([torch.unsqueeze(r, dim=0) for r in result])","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MyViTBlock(nn.Module):\n    def __init__(self, hidden_d, n_heads, mlp_ratio=4):\n        super(MyViTBlock, self).__init__()\n        self.hidden_d = hidden_d\n        self.n_heads = n_heads\n\n        self.norm1 = nn.LayerNorm(hidden_d)\n        self.mhsa = MyMSA(hidden_d, n_heads)\n        self.norm2 = nn.LayerNorm(hidden_d)\n        self.mlp = nn.Sequential(\n            nn.Linear(hidden_d, mlp_ratio * hidden_d),\n            nn.GELU(),\n            nn.Linear(mlp_ratio * hidden_d, hidden_d)\n        )\n\n    def forward(self, x):\n        out = x + self.mhsa(self.norm1(x))\n        out = out + self.mlp(self.norm2(out))\n        return x","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# if __name__ == '__main__':\n#     model = MyViTBlock(hidden_d=4, n_heads=2)\n\n#     x = torch.randn(3, 4, 4)  # Dummy sequences\n#     # print(x)\n#     print(model(x).shape)      ","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = MyViT((1, 28, 28), n_patches=7, n_blocks=2, hidden_d=16, n_heads=2, out_d=10).to(device)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"N_EPOCHS = 30\nLR = 0.01\n\n# Training loop\noptimizer = Adam(model.parameters(), lr=LR)\ncriterion = CrossEntropyLoss()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for epoch in range(N_EPOCHS, desc=\"Training\"):\n        model.train()\n        train_loss = 0.0\n        for batch in train_loader:\n            x, y = batch\n            x, y = x.to(device), y.to(device)\n            predit = model(x)\n            loss = criterion(predit, y)\n\n            train_loss += loss.detach().cpu().item() \n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        print(f\"Epoch {epoch + 1}/{N_EPOCHS} loss: {train_loss/len(train_loader):.6f}\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}